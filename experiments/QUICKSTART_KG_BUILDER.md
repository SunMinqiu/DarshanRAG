# Darshan KG Builder - å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

### æ­¥éª¤1: å®‰è£…ä¾èµ–

```bash
cd /users/Minqiu/DarshanRAG/experiments

# å®‰è£…Pythonä¾èµ–
pip install -r requirements_kg_builder.txt

# æˆ–è€…ä½¿ç”¨uvï¼ˆæ¨èï¼‰
uv pip install -r requirements_kg_builder.txt
```

### æ­¥éª¤2: è¿è¡Œæµ‹è¯•ï¼ˆå¯é€‰ï¼‰

```bash
# éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸ
python test_kg_builder.py

# é¢„æœŸè¾“å‡º:
# ğŸ‰ ALL TESTS PASSED!
```

### æ­¥éª¤3: æ„å»ºçŸ¥è¯†å›¾è°±

```bash
# ä»Darshanæ—¥å¿—æ„å»ºKG
python build_darshan_kg.py \
    --input_path /users/Minqiu/parsed-logs-2025-1-1 \
    --output_path darshan_graph1_2025-1-1.json

# é¢„æœŸè¾“å‡º:
# ğŸ” Searching for Darshan logs in: /users/Minqiu/parsed-logs-2025-1-1
# âœ… Found 150 log file(s)
# ğŸ“„ [1/150] Parsing: ...
# ...
# ğŸ“Š Knowledge Graph Statistics:
#    - Chunks: 150
#    - Entities: 4523
#    - Relationships: 8946
# âœ… Knowledge graph saved successfully!
```

### æ­¥éª¤4: åŠ è½½åˆ°LightRAG

```bash
# è®¾ç½®OpenAI APIå¯†é’¥
export OPENAI_API_KEY='sk-your-api-key-here'

# åŠ è½½KGåˆ°LightRAG
python load_darshan_kg.py \
    --kg_path darshan_graph1_2025-1-1.json \
    --working_dir ./lightrag_storage_2025_1_1

# é¢„æœŸè¾“å‡º:
# ğŸ“‚ Loading KG from: darshan_graph1_2025-1-1.json
# ğŸ“Š KG Statistics:
#    - Chunks: 150
#    - Entities: 4523
#    - Relationships: 8946
# ğŸš€ Initializing LightRAG...
# âœ… LightRAG initialized
# ğŸ“¥ Inserting KG into LightRAG...
# âš ï¸  This will generate embeddings for all entities and relationships.
# â³ This may take a while...
# âœ… KG inserted successfully!
#
# ğŸ” Running Example Queries
# --- Query 1/5 ---
# Q: What jobs are in the knowledge graph?
# ...
```

---

## ğŸ“– è¯¦ç»†è¯´æ˜

### è¾“å…¥æ ¼å¼è¦æ±‚

**æ”¯æŒçš„è¾“å…¥ç±»å‹**:
1. âœ… å•ä¸ª`.txt`æ–‡ä»¶ï¼ˆdarshan-parserè¾“å‡ºï¼‰
2. âœ… åŒ…å«å¤šä¸ª`.txt`æ–‡ä»¶çš„æ–‡ä»¶å¤¹
3. âœ… çˆ¶æ–‡ä»¶å¤¹ï¼ˆä¼šé€’å½’æœç´¢æ‰€æœ‰`.txt`æ–‡ä»¶ï¼‰

**å¦‚ä½•è·å–`.txt`æ ¼å¼çš„Darshanæ—¥å¿—**:

```bash
# ä½¿ç”¨darshan-parserè½¬æ¢.darshanæ–‡ä»¶
darshan-parser --all your_log.darshan > your_log.txt

# æ‰¹é‡è½¬æ¢
for log in *.darshan; do
    darshan-parser --all "$log" > "${log%.darshan}.txt"
done
```

---

## ğŸ¯ å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: å•ä¸ªä½œä¸šåˆ†æ

```bash
# 1. è½¬æ¢å•ä¸ªdarshanæ—¥å¿—
darshan-parser --all job_12345.darshan > job_12345.txt

# 2. æ„å»ºKG
python build_darshan_kg.py \
    --input_path job_12345.txt \
    --output_path job_12345_kg.json

# 3. åŠ è½½å¹¶æŸ¥è¯¢
python load_darshan_kg.py --kg_path job_12345_kg.json
```

### åœºæ™¯2: æ‰¹é‡ä½œä¸šåˆ†æ

```bash
# 1. æ‰¹é‡è½¬æ¢darshanæ—¥å¿—
mkdir parsed_logs
for log in /path/to/darshan/logs/*.darshan; do
    darshan-parser --all "$log" > "parsed_logs/$(basename ${log%.darshan}).txt"
done

# 2. æ„å»ºå¤§è§„æ¨¡KG
python build_darshan_kg.py \
    --input_path parsed_logs/ \
    --output_path batch_analysis_kg.json

# 3. åŠ è½½åˆ°LightRAG
python load_darshan_kg.py \
    --kg_path batch_analysis_kg.json \
    --working_dir ./lightrag_batch_storage
```

### åœºæ™¯3: ç¨‹åºåŒ–æŸ¥è¯¢

```python
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, openai_embed

async def main():
    # åˆå§‹åŒ–LightRAGï¼ˆæŒ‡å‘å·²æœ‰çš„å­˜å‚¨ï¼‰
    rag = LightRAG(
        working_dir='./lightrag_storage_2025_1_1',
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete
    )

    await rag.initialize_storages()

    # è‡ªå®šä¹‰æŸ¥è¯¢
    questions = [
        "Which jobs have the highest I/O bandwidth?",
        "What files are frequently accessed as checkpoints?",
        "Show me jobs with imbalanced I/O across ranks",
        "Which filesystem (Lustre/GPFS/NFS) is used most?"
    ]

    for q in questions:
        print(f"\nQ: {q}")
        result = await rag.aquery(q, param=QueryParam(mode="hybrid"))
        print(f"A: {result}\n")
        print("-" * 70)

    await rag.finalize_storages()

asyncio.run(main())
```

---

## ğŸ” ç”Ÿæˆçš„KGç»“æ„é¢„è§ˆ

æ„å»ºçš„çŸ¥è¯†å›¾è°±åŒ…å«ä»¥ä¸‹èŠ‚ç‚¹å’Œå…³ç³»:

### èŠ‚ç‚¹ç±»å‹

1. **JobèŠ‚ç‚¹**: ä½œä¸šå…ƒæ•°æ®
   - å±æ€§: job_id, start_time, end_time, runtime_sec, nprocs, exeç­‰

2. **ModuleèŠ‚ç‚¹**: I/Oæ¨¡å—ï¼ˆPOSIX, STDIO, MPIIOç­‰ï¼‰
   - å±æ€§: module_name, record_countç­‰

3. **FileRecordèŠ‚ç‚¹**: æ–‡ä»¶è®¿é—®è®°å½•ï¼ˆ**æ ¸å¿ƒæ•°æ®èŠ‚ç‚¹**ï¼‰
   - å±æ€§: file_path, rank, is_shared, file_role_hint
   - **counters_blob**: åŒ…å«æ‰€æœ‰åŸå§‹counteræ•°æ®ï¼ˆå®Œæ•´è¯æ®é“¾ï¼‰

4. **PhaseèŠ‚ç‚¹**: æ—¶é—´æ®µï¼ˆopen/read/write/close/metaï¼‰
   - å±æ€§: t_start, t_end, duration, bytes, iops_est, bw_est

5. **EventAnchorèŠ‚ç‚¹**: æ—¶é—´ç‚¹ï¼ˆfirst_open, last_readç­‰ï¼‰
   - å±æ€§: kind, timestamp, confidence

6. **CounterèŠ‚ç‚¹**ï¼ˆå¯é€‰ï¼‰: å•ä¸ªcounterç´¢å¼•
   - å±æ€§: counter_name, counter_type, value_json

### å…³ç³»ç±»å‹

- `(Job)-[:HAS_MODULE]->(Module)`
- `(Module)-[:HAS_RECORD]->(FileRecord)`
- `(FileRecord)-[:HAS_PHASE]->(Phase)`
- `(Phase)-[:CONTAINS_ANCHOR]->(EventAnchor)`
- `(FileRecord)-[:HAS_COUNTER]->(Counter)`ï¼ˆå¯é€‰ï¼‰

### ç¤ºä¾‹æŸ¥è¯¢ç”¨ä¾‹

```python
# 1. æ€§èƒ½åˆ†ææŸ¥è¯¢
"Which jobs have the lowest I/O bandwidth on shared files?"

# 2. æ¨¡å¼è¯†åˆ«æŸ¥è¯¢
"What are the common checkpoint file access patterns?"

# 3. èµ„æºå®šä½æŸ¥è¯¢
"Which files are accessed by more than 10 jobs?"

# 4. æ—¶é—´åºåˆ—æŸ¥è¯¢
"Show me the I/O timeline for job_12345"

# 5. å¼‚å¸¸æ£€æµ‹æŸ¥è¯¢
"Find jobs with unusually long read phases"
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆæ’å…¥KGå¾ˆæ…¢ï¼Ÿ

**A**: è¿™æ˜¯æ­£å¸¸ç°è±¡ã€‚LightRAGéœ€è¦ä¸ºæ‰€æœ‰å®ä½“å’Œå…³ç³»ç”Ÿæˆembeddingsï¼ˆè°ƒç”¨OpenAI APIï¼‰ã€‚

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨æ›´å¿«çš„embeddingæ¨¡å‹ï¼š`text-embedding-3-small`
- å¢åŠ å¹¶å‘åº¦ï¼ˆä¿®æ”¹LightRAGé…ç½®ï¼‰
- è€ƒè™‘ä½¿ç”¨æœ¬åœ°embeddingæ¨¡å‹

**æ—¶é—´ä¼°ç®—**:
- 150ä¸ªä½œä¸š â†’ çº¦4500ä¸ªå®ä½“ â†’ çº¦10-20åˆ†é’Ÿï¼ˆå–å†³äºAPIé€Ÿåº¦ï¼‰

### Q2: å¦‚ä½•æŸ¥çœ‹ç”Ÿæˆçš„KGå†…å®¹ï¼Ÿ

```bash
# æŸ¥çœ‹KG JSONæ–‡ä»¶
jq . darshan_graph1_2025-1-1.json | less

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
jq '{chunks: (.chunks | length), entities: (.entities | length), relationships: (.relationships | length)}' darshan_graph1_2025-1-1.json

# æŸ¥çœ‹å®ä½“ç±»å‹åˆ†å¸ƒ
jq '[.entities[].entity_type] | group_by(.) | map({type: .[0], count: length})' darshan_graph1_2025-1-1.json
```

### Q3: å¦‚ä½•å¯¼å‡ºåˆ†æç»“æœï¼Ÿ

```python
import asyncio
from lightrag import LightRAG

async def export_data():
    rag = LightRAG(working_dir='./lightrag_storage_2025_1_1')
    await rag.initialize_storages()

    # å¯¼å‡ºä¸ºExcel
    rag.export_data("darshan_analysis.xlsx", file_format="excel")

    # å¯¼å‡ºä¸ºCSV
    rag.export_data("darshan_analysis.csv", file_format="csv")

    await rag.finalize_storages()

asyncio.run(export_data())
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### æ„å»ºé˜¶æ®µä¼˜åŒ–

```bash
# 1. å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶ï¼ˆå¦‚æœæ—¥å¿—å¾ˆå¤šï¼‰
# æš‚ä¸æ”¯æŒï¼Œä½†å¯ä»¥åˆ†æ‰¹å¤„ç†ååˆå¹¶

# 2. åªå¤„ç†æœ€è¿‘çš„æ—¥å¿—
find /path/to/logs -name "*.txt" -mtime -7 | \
    xargs python build_darshan_kg.py --input_path - --output_path recent_logs_kg.json
```

### åŠ è½½é˜¶æ®µä¼˜åŒ–

```python
from lightrag import LightRAG
from lightrag.llm.openai import openai_embed
from lightrag.utils import EmbeddingFunc

# ä½¿ç”¨æ›´å°çš„embeddingæ¨¡å‹
async def faster_embed(texts):
    return await openai_embed(
        texts,
        model="text-embedding-3-small"  # æ›´å¿«ä¸”æ›´ä¾¿å®œ
    )

rag = LightRAG(
    working_dir='./lightrag_storage',
    embedding_func=EmbeddingFunc(
        embedding_dim=1536,
        func=faster_embed
    ),
    embedding_func_max_async=32,  # å¢åŠ å¹¶å‘åº¦
)
```

---

## ğŸ“ è¿›é˜¶ç”¨æ³•

### è‡ªå®šä¹‰Schemaæ‰©å±•

å¦‚æœä½ éœ€è¦æ·»åŠ è‡ªå®šä¹‰èŠ‚ç‚¹/å±æ€§ï¼Œä¿®æ”¹ [build_darshan_kg.py](build_darshan_kg.py) ä¸­çš„ç›¸åº”æ–¹æ³•ã€‚

### æ‰¹é‡æŸ¥è¯¢è„šæœ¬

```python
import asyncio
import json
from lightrag import LightRAG, QueryParam

async def batch_query():
    rag = LightRAG(working_dir='./lightrag_storage_2025_1_1')
    await rag.initialize_storages()

    # ä»æ–‡ä»¶è¯»å–æŸ¥è¯¢åˆ—è¡¨
    with open('queries.txt', 'r') as f:
        queries = [line.strip() for line in f if line.strip()]

    results = []
    for i, query in enumerate(queries, 1):
        print(f"[{i}/{len(queries)}] {query}")
        result = await rag.aquery(query, param=QueryParam(mode="hybrid"))
        results.append({'query': query, 'result': result})

    # ä¿å­˜ç»“æœ
    with open('query_results.json', 'w') as f:
        json.dump(results, f, indent=2)

    await rag.finalize_storages()

asyncio.run(batch_query())
```

---

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£: [README_KG_BUILDER.md](README_KG_BUILDER.md)
2. è¿è¡Œæµ‹è¯•éªŒè¯ç¯å¢ƒ: `python test_kg_builder.py`
3. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ ¼å¼: `head -50 your_log.txt`

---

**Happy Querying! ğŸš€**

#!/usr/bin/env python3
"""
Darshan KG Builder V2
Converts Darshan signal extraction output (v2.4+) to LightRAG custom KG format.

Architecture V2:
- 6 node types: Application, Job, Module, Record, File, FileSystem
- Relationships: Application→Job, Job→Module, Module→Record, Record→File, File→FileSystem, Job→FileSystem
- Description and chunks left empty (will be generated by template extraction)
- Record = incident (minimum retrievable unit)
"""

import re
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import defaultdict


class DarshanKGBuilderV2:
    """Build Knowledge Graph V2 from Darshan signal extraction output."""

    def __init__(self):
        # Job-level metadata
        self.job_info = {}

        # Mount table: {mount_pt: fs_type} - stored as Job attribute, not edges
        self.mount_table = {}

        # Module-level aggregates: {module_name: {metric: value}}
        self.modules = {}

        # Records: [{module, rank, record_id, file_name, signals, ...}]
        self.records = []

        # Files: {file_path_norm: {file_path_raw, mount_pt, fs_type}}
        self.files = {}

        # FileSystems actually touched by records: {(mount_pt, fs_type): {mount_pt, fs_type}}
        self.filesystems_touched = {}

        # Applications: {exe: {exe}}
        self.applications = {}

    def parse_darshan_signal_file(self, file_path: str) -> None:
        """Parse a single Darshan signal extraction output file (v2.4+)."""
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

        # Parse job metadata from header
        self._parse_job_metadata(content)

        # Parse mount table
        self._parse_mount_table(content)

        # Parse JOB level aggregates
        self._parse_job_aggregates(content)

        # Parse all module sections
        for module_name in ['HEATMAP', 'POSIX', 'STDIO', 'MPIIO']:
            self._parse_module_section(content, module_name)

    def _parse_job_metadata(self, content: str) -> None:
        """Extract job-level metadata from header."""
        patterns = {
            'job_id': r'#\s+jobid:\s+(\d+)',
            'uid': r'#\s+uid:\s+(\d+)',
            'nprocs': r'#\s+nprocs:\s+(\d+)',
            'runtime': r'#\s+run time:\s+([\d.]+)',
            'exe': r'#\s+exe:\s+(\w+)',
            # 'start_time': r'#\s+start_time:\s+(\d+)',
            # 'end_time': r'#\s+end_time:\s+(\d+)',
            'start_time_asci': r'#\s+start_time_asci:\s+(.+)',
            'end_time_asci': r'#\s+end_time_asci:\s+(.+)',
        }

        for key, pattern in patterns.items():
            match = re.search(pattern, content)
            if match:
                value = match.group(1)
                # Convert to appropriate type
                # if key in ['job_id', 'uid', 'nprocs', 'start_time_asci', 'end_time_asci']:
                if key in ['job_id', 'uid', 'nprocs']:
                    self.job_info[key] = int(value)
                elif key in ['runtime']:
                    self.job_info[key] = float(value)
                else:
                    self.job_info[key] = value

        # Register application
        if 'exe' in self.job_info:
            exe = self.job_info['exe']
            if exe not in self.applications:
                self.applications[exe] = {'exe': exe}

    def _parse_mount_table(self, content: str) -> None:
        """Parse mount table and store as Job attribute (not entities)."""
        mount_pattern = r'#\s+mount entry:\s+(.+?)\s+(\w+)'
        for match in re.finditer(mount_pattern, content):
            mount_pt = match.group(1)
            fs_type = match.group(2)
            self.mount_table[mount_pt] = fs_type

    def _parse_job_aggregates(self, content: str) -> None:
        """Parse JOB level aggregates."""
        job_agg_pattern = r'JOB\s+(\w+)\s+([\d.]+|NA\(.*?\))'
        for match in re.finditer(job_agg_pattern, content):
            metric = match.group(1)
            value, na_reason = self._convert_value(match.group(2))
            self.job_info[metric] = value
            if na_reason:
                self.job_info[f"{metric}_na_reason"] = na_reason

    def _parse_module_section(self, content: str, module_name: str) -> None:
        """Parse a module section (HEATMAP, POSIX, STDIO, MPIIO)."""
        # Find module section - match with flexible spacing
        module_pattern = rf'# \*+\n#\s+{module_name} module - Derived Signals\n# \*+'
        module_match = re.search(module_pattern, content)
        if not module_match:
            return

        # Extract this module's content section (from header to next module or EOF)
        module_start = module_match.start()

        # Find next module section
        next_module_pattern = r'# \*+\n#\s+\w+ module - Derived Signals\n# \*+'
        next_match = None
        for match in re.finditer(next_module_pattern, content[module_start + len(module_match.group(0)):]):
            next_match = match
            break

        if next_match:
            module_end = module_start + len(module_match.group(0)) + next_match.start()
        else:
            module_end = len(content)

        module_content = content[module_start:module_end]

        # Parse module-level aggregates (search in full content for MODULE_AGG/PERF lines)
        self._parse_module_aggregates(content, module_name)

        # Parse all records in THIS module section only
        self._parse_module_records(module_content, module_name)

    def _parse_module_aggregates(self, content: str, module_name: str) -> None:
        """Parse module-level aggregates and performance signals."""
        # MODULE_AGG pattern
        agg_pattern = rf'{module_name}\s+MODULE_AGG\s+(\w+)\s+([\d.]+|NA\(.*?\))'
        # MODULE_PERF pattern
        perf_pattern = rf'{module_name}\s+MODULE_PERF\s+(\w+)\s+([\d.]+|NA\(.*?\))'

        if module_name not in self.modules:
            self.modules[module_name] = {}

        for match in re.finditer(agg_pattern, content):
            metric = match.group(1)
            value, na_reason = self._convert_value(match.group(2))
            self.modules[module_name][metric] = value
            if na_reason:
                self.modules[module_name][f"{metric}_na_reason"] = na_reason

        for match in re.finditer(perf_pattern, content):
            metric = match.group(1)
            value, na_reason = self._convert_value(match.group(2))
            self.modules[module_name][metric] = value
            if na_reason:
                self.modules[module_name][f"{metric}_na_reason"] = na_reason

    def _parse_module_records(self, content: str, module_name: str) -> None:
        """Parse all records in a module section."""
        # Record header pattern: # Record: 11610284057069735693, rank=-1, file=/home/3079452805, mount=/home, fs=lustre
        record_header_pattern = r'# Record:\s+(\d+),\s+rank=(-?\d+),\s+file=([^,]+)(?:,\s+mount=([^,]+))?(?:,\s+fs=(\w+))?'

        # Find all record headers
        record_matches = list(re.finditer(record_header_pattern, content))

        for i, match in enumerate(record_matches):
            record_id = match.group(1)
            rank = int(match.group(2))
            file_name = match.group(3)
            mount_pt = match.group(4) if match.group(4) else "UNKNOWN"
            fs_type = match.group(5) if match.group(5) else "UNKNOWN"

            # Find the record block (from current match to next record or end)
            start_pos = match.end()
            if i + 1 < len(record_matches):
                end_pos = record_matches[i + 1].start()
            else:
                # Find next module section or end
                next_module_match = re.search(r'# \*{70}\n#\s+\w+ module', content[start_pos:])
                if next_module_match:
                    end_pos = start_pos + next_module_match.start()
                else:
                    end_pos = len(content)

            record_block = content[start_pos:end_pos]

            # Parse signals in this record
            signals = self._parse_record_signals(record_block)

            # Create record entry
            record = {
                'module': module_name,
                'rank': rank,
                'record_id': record_id,
                'file_name': file_name,
                'mount_pt': mount_pt,
                'fs_type': fs_type,
                **signals
            }

            self.records.append(record)

            # Register file if it's a real file (not heatmap:POSIX/STDIO)
            if not file_name.startswith('heatmap:'):
                file_path_norm = self._normalize_file_path(file_name)
                if file_path_norm not in self.files:
                    self.files[file_path_norm] = {
                        'file_path_raw': file_name,
                        'file_path_norm': file_path_norm,
                        'mount_pt': mount_pt,
                        'fs_type': fs_type
                    }

            # Register filesystem touched by this record
            if fs_type != "UNKNOWN" and mount_pt != "UNKNOWN":
                fs_key = (mount_pt, fs_type)
                if fs_key not in self.filesystems_touched:
                    self.filesystems_touched[fs_key] = {
                        'mount_pt': mount_pt,
                        'fs_type': fs_type
                    }

    def _parse_record_signals(self, record_block: str) -> Dict[str, Any]:
        """Parse SIGNAL_* lines in a record block."""
        signals = {}
        signal_pattern = r'SIGNAL_(\w+)\s+([\d.eE+-]+|NA\(.*?\))'

        for match in re.finditer(signal_pattern, record_block):
            signal_name = match.group(1).lower()
            signal_value, na_reason = self._convert_value(match.group(2))
            signals[signal_name] = signal_value
            if na_reason:
                signals[f"{signal_name}_na_reason"] = na_reason

        return signals

    def _convert_value(self, value_str: str) -> tuple[Any, Optional[str]]:
        """
        Convert string value to appropriate type.
        Returns (value, na_reason) tuple.
        - If valid number: (number, None)
        - If NA: (None, reason)
        """
        if value_str.startswith('NA('):
            # Extract reason from NA(reason)
            reason = value_str[3:-1] if value_str.endswith(')') else value_str[3:]
            return (None, reason)

        try:
            # Try int first
            if '.' not in value_str and 'e' not in value_str.lower():
                return (int(value_str), None)
            else:
                return (float(value_str), None)
        except ValueError:
            return (value_str, None)

    def _normalize_file_path(self, file_path: str) -> str:
        """Normalize file path for unique identification."""
        return file_path.replace('/', '_').replace('.', '_')

    def parse_darshan_directory(self, directory: str) -> None:
        """Recursively parse all txt files in directory."""
        dir_path = Path(directory)
        if not dir_path.exists():
            raise FileNotFoundError(f"Directory {directory} does not exist")

        txt_files = list(dir_path.rglob("*.txt"))
        if not txt_files:
            print(f"Warning: No txt files found in {directory}")
            return

        print(f"Found {len(txt_files)} txt files")
        for txt_file in txt_files:
            print(f"Parsing {txt_file}...")
            self.parse_darshan_signal_file(str(txt_file))

    def build_lightrag_kg(self, source_id: str = "darshan-logs", file_path: str = "") -> Dict:
        """Build LightRAG custom KG format from parsed data."""
        chunks = []
        entities = []
        relationships = []

        # Validate that we have parsed data
        if not self.job_info:
            raise ValueError("No job metadata found. Did you parse a file first?")

        job_id = self.job_info.get('job_id', 'unknown')
        exe = self.job_info.get('exe', 'unknown')

        # ===== A) Application Entity =====
        app_id = f"App_{exe}"
        app_entity = {
            "entity_name": app_id,
            "entity_type": "APPLICATION",
            "description": "",  # Left empty per V2 requirements
            "source_id": source_id,
            "file_path": file_path
        }
        # Add custom attribute
        app_entity["exe"] = exe
        entities.append(app_entity)

        # ===== B) Job Entity =====
        job_entity_id = f"Job_{job_id}"
        job_entity = {
            "entity_name": job_entity_id,
            "entity_type": "JOB",
            "description": "",  # Left empty per V2 requirements
            "source_id": source_id,
            "file_path": file_path
        }
        # Add all job attributes (custom fields after standard fields)
        job_entity["job_id"] = job_id
        for key, value in self.job_info.items():
            if key not in ['job_id', 'exe']:  # Exclude exe - it's only in Application
                job_entity[key] = value

        # Add mount table as Job attribute (not edges)
        if self.mount_table:
            job_entity['mount_table'] = self.mount_table

        # Add job_io_summary (empty for now)
        job_entity['job_io_summary'] = ""

        entities.append(job_entity)

        # Edge: Application → Job (HAS_JOB)
        relationships.append({
            "src_id": app_id,
            "tgt_id": job_entity_id,
            "description": "",
            "keywords": "application job executable",
            "weight": 1.0,
            "source_id": source_id,
            "file_path": file_path
        })

        # ===== C) Module Entities =====
        module_entity_ids = {}
        for module_name, module_attrs in self.modules.items():
            module_id = f"{job_id}::{module_name}"
            module_entity_ids[module_name] = module_id

            module_entity = {
                "entity_name": module_id,
                "entity_type": "MODULE",
                "description": "",
                "source_id": source_id,
                "file_path": file_path
            }
            # Add custom attributes
            module_entity["module_name"] = module_name
            # Add all module-level aggregates
            for key, value in module_attrs.items():
                module_entity[key] = value

            entities.append(module_entity)

            # Edge: Job → Module (HAS_MODULE)
            relationships.append({
                "src_id": job_entity_id,
                "tgt_id": module_id,
                "description": "",
                "keywords": "job module io_layer",
                "weight": 1.0,
                "source_id": source_id,
                "file_path": file_path
            })

        # ===== F) FileSystem Entities (only those touched by records) =====
        fs_entity_ids = {}
        for (mount_pt, fs_type), fs_attrs in self.filesystems_touched.items():
            fs_id = f"FS_{fs_type}_{self._normalize_file_path(mount_pt)}"
            fs_entity_ids[(mount_pt, fs_type)] = fs_id

            entities.append({
                "entity_name": fs_id,
                "entity_type": "FILESYSTEM",
                "description": "",
                "source_id": source_id,
                "file_path": file_path,
                "mount_pt": mount_pt,
                "fs_type": fs_type
            })

            # Edge: Job → FileSystem (TOUCH_FILESYSTEM)
            # Only create edge for filesystems actually touched by records
            relationships.append({
                "src_id": job_entity_id,
                "tgt_id": fs_id,
                "description": "",
                "keywords": "job filesystem storage",
                "weight": 1.0,
                "source_id": source_id,
                "file_path": file_path
            })

        # ===== E) File Entities =====
        file_entity_ids = {}
        for file_path_norm, file_attrs in self.files.items():
            file_id = f"File_{file_path_norm}"
            file_entity_ids[file_path_norm] = file_id

            entities.append({
                "entity_name": file_id,
                "entity_type": "FILE",
                "description": "",
                "source_id": source_id,
                "file_path": file_path,
                "file_path_raw": file_attrs['file_path_raw'],
                "file_path_norm": file_path_norm,
                "mount_pt": file_attrs['mount_pt'],
                "fs_type": file_attrs['fs_type']
            })

            # Edge: File → FileSystem (ON_FILESYSTEM)
            fs_type = file_attrs['fs_type']
            mount_pt = file_attrs['mount_pt']
            if fs_type != "UNKNOWN" and mount_pt != "UNKNOWN":
                fs_key = (mount_pt, fs_type)
                if fs_key in fs_entity_ids:
                    relationships.append({
                        "src_id": file_id,
                        "tgt_id": fs_entity_ids[fs_key],
                        "description": "",
                        "keywords": "file filesystem storage",
                        "weight": 1.0,
                        "source_id": source_id,
                        "file_path": file_path
                    })

        # ===== D) Record Entities =====
        for record in self.records:
            record_id = f"{job_id}::{record['module']}::{record['record_id']}::rank{record['rank']}"

            record_entity = {
                "entity_name": record_id,
                "entity_type": "RECORD",
                "description": "",
                "source_id": source_id,
                "file_path": file_path,
                "record_id": record['record_id'],
                "rank": record['rank'],
                "file_name": record['file_name'],
                "mount_pt": record['mount_pt'],
                "fs_type": record['fs_type'],
            }

            # Add all signal attributes
            for key, value in record.items():
                if key not in ['module', 'rank', 'record_id', 'file_name', 'mount_pt', 'fs_type']:
                    record_entity[key] = value

            entities.append(record_entity)

            # Edge: Module → Record (HAS_RECORD)
            module_name = record['module']
            if module_name in module_entity_ids:
                relationships.append({
                    "src_id": module_entity_ids[module_name],
                    "tgt_id": record_id,
                    "description": "",
                    "keywords": "module record incident",
                    "weight": 1.0,
                    "source_id": source_id,
                    "file_path": file_path
                })

            # Edge: Record → File (ON_FILE)
            file_name = record['file_name']
            if not file_name.startswith('heatmap:'):
                file_path_norm = self._normalize_file_path(file_name)
                if file_path_norm in file_entity_ids:
                    relationships.append({
                        "src_id": record_id,
                        "tgt_id": file_entity_ids[file_path_norm],
                        "description": "",
                        "keywords": "record file io_access",
                        "weight": 1.0,
                        "source_id": source_id,
                        "file_path": file_path
                    })

        # Build final KG structure
        kg = {
            "chunks": chunks,  # Left empty per V2 requirements
            "entities": entities,
            "relationships": relationships
        }

        return kg


def main():
    parser = argparse.ArgumentParser(
        description="Convert Darshan signal extraction output to LightRAG custom KG format (V2)"
    )
    parser.add_argument(
        "-i", "--input",
        required=True,
        help="Input: txt file, directory, or parent directory"
    )
    parser.add_argument(
        "-o", "--output",
        required=True,
        help="Output JSON file path"
    )
    parser.add_argument(
        "--source-id",
        default="darshan-logs",
        help="Source ID for the KG (default: darshan-logs)"
    )

    args = parser.parse_args()

    # Initialize builder
    builder = DarshanKGBuilderV2()

    # Parse input
    input_path = Path(args.input)
    if input_path.is_file():
        print(f"Parsing single file: {input_path}")
        builder.parse_darshan_signal_file(str(input_path))
    elif input_path.is_dir():
        print(f"Parsing directory: {input_path}")
        builder.parse_darshan_directory(str(input_path))
    else:
        print(f"Error: {input_path} is neither a file nor a directory")
        return 1

    # Build KG
    print("\nBuilding Knowledge Graph...")
    kg = builder.build_lightrag_kg(
        source_id=args.source_id,
        file_path=str(input_path)
    )

    # Save to JSON
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(kg, f, indent=2)

    # Print statistics
    print(f"\n{'='*60}")
    print("Knowledge Graph Statistics:")
    print(f"{'='*60}")
    print(f"Total entities: {len(kg['entities'])}")
    print(f"Total relationships: {len(kg['relationships'])}")

    entity_counts = defaultdict(int)
    for entity in kg['entities']:
        entity_counts[entity['entity_type']] += 1

    print("\nEntity breakdown:")
    for entity_type, count in sorted(entity_counts.items()):
        print(f"  {entity_type}: {count}")

    print(f"\nOutput saved to: {args.output}")

    return 0


if __name__ == "__main__":
    exit(main())
